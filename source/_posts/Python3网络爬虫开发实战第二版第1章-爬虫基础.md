---
title: Python3网络爬虫开发实战第2版第一章爬虫基础
date: 2022-1-28 15:43:07
---

本章我们将了解到HTTP原理、网页的基础知识、爬虫的基本原理、Cookie的基本原理、多进程和多线程的基本原理等，了解这些内容有助于我们更好地理解和编写网络爬虫相关的程序

## HTTP基本原理

### URI和URL

URL: Uniform Resource Location(统一资源定位符)

URI: Uniform Resource Identifier(统一资源标识符)

URL是大家所熟知的，它指向网络服务器上的某个资源，而其实URI有两个子类，一个是上面提到的URL，还有一个叫做URN(Uniform Resource Name)，URN只为资源命名而不指定如何定位资源，也就是说**URI包括了URL和URN**；

### URL的基本组成格式

```
scheme://[username:password@]hostname[:port][path][;parameters][?query][#fragment]
```

scheme: 协议，常用的协议有http、https、ftp等，另外，scheme也常被称作protocol，二者都代表协议的意思

username、password: 用户名和密码，在某些情况下URL需要提供用户名和密码才能访问

hostname: 主机地址，或IP地址

port: 端口，这是服务器设定的服务端口，但是有些URL中没有端口信息，这是使用了默认的端口，http协议的默认端口是80，https协议的默认端口是443

path: 路径，指的是网络资源在服务器中的指定位置

parameters: 参数，用来指定访问某个资源时的附加信息，这个用的比较少

query: 查询，用来查询某类资源，如果有多个查询，则用&隔开

fragment: 片段，它是对资源描述的部分补充，可以理解为资源内部的书签

### HTTP和HTTPS

在爬虫中，我们主要接触到的协议通常是基于http或https协议的，因此这里首先了解下这两个协议:

HTTP的全称是Hypertext Transfer Protocol，中文名为超文本传输协议，其作用是**把超文本数据从网络传输到本地浏览器，能够保证高效而准确地传输超文本文档**，目前被人们广泛使用的是HTTP1.1版本，当然，现在也有不少网站支持HTTP2.0

HTTPS的全称是`Hyper Text Transfer Protocol over SecureSocket Layer`，是以安全为目标的HTTP通道，简单讲就是**HTTP的安全版，即在HTTP下加入SSL层**，简称HTTPS

HTTPS的安全基础是SSL，因此通过该协议传输的内容都是经过SSL加密的，SSL的主要作用有以下两种:

+ 建立一个信息安全通道，保证数据传输的安全性
+ 确认网站的真实性，凡是使用了HTTPS协议的网站，都可以通过单击浏览器地址栏的锁头编制来查看网站认证之后的真实信息，此外还可以通过CA机构颁发的安全签章来查询

HTTP和HTTPS协议都属于计算机网络中的应用层协议，其下层是基于TCP协议实现的，TCP协议属于计算机网络中的传输层协议，包括建立连接时的三次握手和断开时的四次挥手等过程

### HTTP请求过程

在浏览器地址栏输入一个URL，按下回车之后便可观察到对应的页面内容，实际上，这个过程是浏览器先向网站所在的服务器发送一个请求，网站服务器接收到请求后对其处理和解析，然后返回对应的响应，接着传回浏览器；由于响应里包含页面的源代码等响应内容，所以浏览器再对其进行解析，便将网页呈现出来

关于浏览器开发者工具的使用[点此跳转](http://kest.club/2022/01/19/Chrome%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/)

### 请求

请求，英文为Request，由客户端发往服务器，分为四部分内容: 请求方法(Request Method)、请求的网址(Request URL)，请求头(Request head)、请求体(Request body)

#### 请求方法

请求方法，用于标识客户端请求服务端的方式，常见的请求方法，有两种: GET和POST，在浏览器中直接输入URL并回车，便发起了一个GET请求，请求参数会直接包含到URL里；POST请求大多会在提交表单时发起，例如，对于一个登录表单，输入用户名和密码后，单击登录按钮，这时通常会发起一个POST请求，其数据通常会以表单的形式传输，而不会体现在URL中

##### GET和POST的区别

+ GET请求中的参数包含在URL里面，数据可以在URL中看到；而POST请求的URL不会包含这些数据，数据都是通过表单形式传输，会包含在请求体中
+ GET请求提交的数据最多只有1024字节，POST方式则没有限制

### 响应

响应，即Response，由服务器返回给客户端，可以分为三部分: 响应状态码(HTTP Status Code)，响应头(Response head)，响应体(Response body)

#### 响应状态码

详情[点击这里](http://kest.club/2022/01/19/%E5%85%B3%E4%BA%8EHTTP%E7%8A%B6%E6%80%81%E7%A0%81/)

## Web网页基础

### 网页的组成

网页可以分为三大部分: HTML、CSS和JS

+ HTML: HTML的全称为[超文本标记语言](https://baike.baidu.com/item/超文本标记语言/6972570)，是一种[标记语言](https://baike.baidu.com/item/标记语言/5964436)。它包括一系列[标签](https://baike.baidu.com/item/标签/2440469)．通过这些标签可以将网络上的[文档](https://baike.baidu.com/item/文档/1009768)格式统一，使分散的[Internet](https://baike.baidu.com/item/Internet/272794)资源连接为一个逻辑整体。HTML文本是由HTML命令组成的描述性[文本](https://baike.baidu.com/item/文本/5443630)，HTML命令可以说明[文字](https://baike.baidu.com/item/文字/612910)，[图形](https://baike.baidu.com/item/图形/773307)、[动画](https://baike.baidu.com/item/动画/206564)、[声音](https://baike.baidu.com/item/声音/33686)、[表格](https://baike.baidu.com/item/表格/3371820)、[链接](https://baike.baidu.com/item/链接/2665501)等
+ CSS: 层叠样式表(英文全称：Cascading Style Sheets)是一种用来表现[HTML](https://baike.baidu.com/item/HTML)（[标准通用标记语言](https://baike.baidu.com/item/标准通用标记语言/6805073)的一个应用）或[XML](https://baike.baidu.com/item/XML)（标准通用标记语言的一个子集）等文件样式的计算机语言。CSS不仅可以静态地修饰网页，还可以配合各种脚本语言动态地对网页各元素进行格式化
+ JavaScript: （简称“JS”） 是一种具有函数优先的轻量级，解释型或即时编译型的[编程语言](https://baike.baidu.com/item/编程语言/9845131)。虽然它是作为开发[Web](https://baike.baidu.com/item/Web/150564)页面的[脚本语言](https://baike.baidu.com/item/脚本语言/1379708)而出名，但是它也被用到了很多非[浏览器](https://baike.baidu.com/item/浏览器/213911)环境中，JavaScript 基于原型编程、多范式的动态脚本语言，并且支持[面向对象](https://baike.baidu.com/item/面向对象/2262089)、命令式、声明式、[函数](https://baike.baidu.com/item/函数/301912)式编程范式

### 网页的结构

看示例代码:

```html
<!DOCTYPE html>        <!-- 声明文档。定义html -->
<html lang="en">         <!-- 元素是页面的根元素 -->
<head>                     <!-- 元素包含文档的元数据 -->
    <meta charset="UTF-8">   <!-- 定义网页编码格式 -->
    <title>第一个项目</title>  <!-- 元素描述了文档的标题-->
</head>
<body><!--元素包含了页面可以看见的内容。定义文档主体。-->
    <p>这个p是段落。可以把很多文字放到里面去。比如这是一段文字。</p>
    <p>这是另一个段落。</p>
    <h1>这是h1标题</h1><!-- 通过<h1>-<h6>标签来定义 -->
    <h2>这是h2标题</h2>
    <h3>这是h3标题</h3>
    <h4>这是h4标题</h4>
    <h5>这是h5标题</h5>
    <h6>这是h6标题</h6>
    <hr><!--定义水平线-->
    <p>这是一个段落。插入图片</p>
    <a href="www.baidu.com">这是跳转到的百度链接</a>  <!--<a>标签来定义。在 href 属性中指定链接的地址。-->
    <br><!--换行-->
    <img src="images/one.jpg" alt="" width="500" height="500">   <!--图像是通过标签 <img> 来定义的。 -->
    <img src="images/two.jpg" alt="" width="500" height="500">
    <hr>
<!--文本格式化的标签-->
    <b>这是一句话。定义粗体</b>
    <em>这是一句话。定义着重文字</em>
    <i>这是一句话。定义斜体</i>
    <small>这是一句话。定义小字号</small>
    <strong>这是一句话。定义加重语气</strong>
    <p>插入<sub>这是一句话</sub>定义下标字</p>
    <p>插入<sup>这是一句话</sup>定义上标字</p>
    <ins>这是一句话。定义插入字</ins>
    <del>这是一句话。定义删除字</del>
<!-- 计算机输出标签 -->
    <hr>
    <p>这是计算机输出标签</p>
    <code >#定义计算机代码print("hello world!")
    </code>
    <br>
    <kbd>定义键盘码</kbd>
    <br>
    <samp>定义计算机代码样本</samp>
    <br>
    <var>定义变量</var>
    <pre>定义预格式文本</pre>
    <p><b>注释：</b>这些标签常用于显示计算机/编程代码。</p>
<!-- HTML 引文 引用 标签定义 -->
    <hr>
    <abbr title="">定义缩写</abbr>
    <address>定义地址</address>
    <p><bdo dir="rtl">该段落文字从右到左显示。</bdo></p>
    <blockquote>定义长的引用</blockquote>
    <q>定义短的引用</q>
    <br>
    <cite>定义引用、引证</cite>
    <br>
    <dfn>定义一个定义项目</dfn>
</body>
</html>
```

### 节点树及节点间的关系

在HTML中，所有标签定义的内容都是节点，这些节点构成一个HTML节点树，也叫HTML DOM树；先来看下什么是DOM，DOM是W3C(万维网联盟)的标准，英文全称是Document Object Model，即文档对象模型，它定义了访问HTML和XML文档的标准，根据W3C的HTML DOM标准，HTML文档中的所有内容都是节点

* 整个网站文档是一个文档节点
* 每个html标签对应一个根节点，即上例中的html标签，它属于一个根节点
* 节点内的文本是文本节点，比如a节点代表一个超链接，它内部的文本也被认为是一个文本节点
* 每个节点的属性是属性节点，比如a节点有一个href属性，它就是一个属性节点
* 注释是注释节点，在HTML中有特殊的语法会被解析为注释，它也会对应一个节点

### 选择器

#### CSS选择器

详情请见: https://www.w3school.com.cn/cssref/css_selectors.asp

#### Xpath

详情请见: https://www.w3school.com.cn/xpath/index.asp

## 爬虫的基本原理

### 爬虫概述

简单点讲，爬虫就是**获取网页并提取和保存信息的自动化程序**

#### 获取网页

爬虫的工作首先是获取网页的源代码，源代码里包含网页的部分有用信息，所以只要获取源代码，就可以从中提取想要的信息了，Python提供了许多库，可以帮助我们获取源代码，比如urllib，requests等，我们可以用这些库完成HTTP请求操作，除此之外，请求和响应都可以用类库提供的数据结构来表示，因此得到相应之后只需要解析数据结构中的body部分，即可得到网页的源代码

#### 提取信息

提取网页的源代码之后，接下来的就是分析源代码，从中提取我们想要的数据，首先，最通用的提取方式是采用正则表达式，这是一个万能的方法，但是构造正则表达式的过程比较复杂且容易出错；另外，由于网页结构具有一定的规则，所以还有一些库是根据网页节点属性、CSS选择器或Xpath来提取网页信息的，如BeautifulSoup、pyquery，lxml等，使用这些库，可以高效地从源代码中提取网页信息，如节点地属性，文本值等

#### 保存数据

提取数据之后，我们一般会将提取到的数据保存到某处以便后续使用，保存数据的形式多种多样，可以简单保存为TXT文本或者JSON文本，也可以保存到数据库，如MySQL和MongoDB等，还可保存到远程服务器如借助SFTP进行操作等

#### 自动化程序

自动化程序的意思是爬虫可以代替人来完成上述操作，我们当然可以手动提取网页中的信息，但是当量特别大或者想快速大量获取数据的时候，肯定还是借助程序快，爬虫就是代替我们完成爬取工作的自动化程序，它可以在爬取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行

### 能爬怎样的数据

网页中存在各种各样的信息，最常见的便是常规网页，这些网页对应着HTML代码，而最常抓取的便是HTML源代码

另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串(其中API接口大多采用这种方式)，这种格式的数据方便传输和解析，爬虫同样可以抓取这些数据，而且数据提取会更加方便；

网页中还包含着各种二进制数据，如图片、视频音频文件，利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名

除了上述数据，网页中还有各种扩展名文件，如CSS、Javascript和配置文件等

### Javascript渲染的页面

对于Javascript渲染的网页，我们可以分析源代码后台Ajax接口，也可使用Selenium、Splash、Pyppeteer、Playwright这样的库来模拟Javascript渲染

## Session和Cookie

### 神秘的凭证

很多页面是需要登录之后才可以查看的，按照一般的逻辑，输入用户名和密码登陆网站，肯定是拿到了一种类似凭证的东西，有了这个凭证，才能保持登录的状态，访问那些登录之后才能看到的页面

那么这种神秘的凭证到底是什么呢，其实它就是Session和Cookie共同产生的结果

### 无状态HTTP

我们需要知道HTTP的一个特点: **无状态**

什么叫无状态呢，也就是说**HTTP协议对事物处理是没有记忆能力的，或者说服务并不知道客户端处于什么状态**，这时，两种用于保持HTTP连接状态的技术出现了，分别是Session和Cookie；**Session在服务端**，也就是网站的服务器，用来保存用户的Session信息，**Cookie在客户端**，也可以理解为在浏览器端，有了Cookie，浏览器在下次访问相同网页时就会自动附带上它，并发送给服务器，服务器通过识别Cookie鉴定出是哪个用户在访问，然后判断此用户是否处于登录状态，并返回对应的响应

### Session

Session译为会话，其本意是指有始有终的一系列动作、消息，例如打电话时，从拿起电话拨号到挂断电话之间的一系列过程就可以称为一个Session

而在Web中，Session对象用来存储特定用户Session所需的属性及配置信息，这样，当用户在应用程序的页面之间跳转时，存储在Session对象中的变量将不会丢失，会在整个Session中一直保存下去；当用户请求来自应用程序的页面时，如果该用户还没有Session，那么Web服务器将自动创建一个Session对象，当Session过期或被放弃后，服务器将终止该Session

### Cookie

值某些网站为了鉴别用户身份、进行Session跟踪而存储在用户本地终端上的数据

#### Session维持

如何利用Cookie保持状态呢，在客户端第一次请求服务器时，服务器会返回一个响应头中带有Set-Cookie字段的响应给客户端，这个字段用来标记用户；客户端浏览器会把Cookie保存起来，当下一次请求相同的网站时，把保存的Cookie放到请求头中一起交给服务器；Cookie中携带着Session ID相关信息，服务器通过检查Cookie即可找到对应的Session，继而通过判断Session辨认用户状态；如果Session当前是有效的，就证明用户处于登录状态，此时服务器返回登录之后才可以查看的网页内容，浏览器再进行解析即可

反之，如果传给服务器的Cookie是无效的，或者Session已经过期了，客户端将不能继续访问页面

Cookie和Session需要配合，一个在客户端，一个在服务端，二者共同协作，就实现了登录控制

#### 属性结构

接下来，我们看看Cookie都包含哪些内容:

Name: Cookie的名称，Cookie一旦创建，名称便不可更改

Value: Cookie的值，如果值为Unicode字符，则需要为字符编码；如果值为二进制数据，则需要使用base64编码

Domain: 指定可以访问该Cookie的域名，例如设置Domain为.zhihu.com，表示所有以.zhihu.com结尾的域名都可以访问该Cookie

Path: Cookie的使用路径，如果设置为/path/，则只有该路径为/path/的页面才可以访问该Cookie，如果设置为/，则本域名下的所有页面都可以访问该Cookie

Max-Age: Cookie失效的时间，单位为秒，常和Expires一起使用，通过此属性可以计算出Cookie的有效时间，Max-Age如果为正数，则表示Cookie在Max-Age秒之后失效，如果为负数，则Cookie在关闭浏览器时失效，而且浏览器不会以任何形式保存该Cookie

Size: Cookie的大小

HTTP: Cookie的httponly属性，若此属性为true，则只有在HTTP Headers中才会带有此Cookie的信息，而不能通过document.cookie来访为此Cookie

Secure: 是否允许使用安全协议传输Cookie，安全协议有HTTPS和SSL等，使用这些协议在网络上传输数据之前会先将数据加密，其默认值为false

#### 会话Cookie和持久Cookie

会话Cookie就是把Cookie放在浏览器内存里，关闭浏览器之后，Cookie即失效，持久Cookie则会把Cookie保存在客户端的硬盘中，下次还可以继续使用，用于长久保持用户的登录状态

其实没有会话Cookie和持久Cookie之说，只是Maxage或者Expires字段决定了Cookie失效的时间

因此，一些持久化登录的网站实际上就是把Cookie的有效时间和Session有效期设置得比较长

#### 常见误区

关闭浏览器不会删除Session，关闭浏览器可能会删除会话Cookie，所以就找不到Session ID，如果是持久Cooike就能找到Session ID

### 代理的基本原理

#### 基本原理

代理实际上就是指代理服务器，形象点说代理就是网络信息的中转站

#### 代理的作用

+ 突破自身IP的限制，访问一些平时不能访问的站点
+ 访问一些单位或团体的内部资源
+ 提高访问速度，通常代理服务器会设置一个较大的硬盘缓冲区，当有外界信息通过时，会同时将其保存到自己的缓冲区中，当其他用户访问相同的信息时，直接从缓冲区中取出信息，所以提高了访问速度
+ 隐藏真实的IP

#### 爬虫代理

爬虫中使用代理，可以隐藏我们真实的IP，让服务器误以为是代理服务器在请求自己，这样在爬取过程中不断更换代理，就可以实现避免IP被封锁，达到很好的爬取效果

#### 代理分类

##### 根据协议区分

+ FTP代理服务器: 主要用于访问FTP服务器，一般有上传下载以及缓存功能，端口一般我为21、2121等
+ HTTP代理服务器: 主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80，8080
+ SSL/TLS代理: 主要用于访问加密网站，一般有SSL或TLS加密功能(最高支持128位加密强度)，端口一般为443
+ RTSP代理: 主要用于Realplyer访问流媒体服务器，一般有缓存功能，端口一般为554
+ Telnet代理: 主要用于Telnet远程控制(黑客入侵计算机时常用于隐藏身份)，端口一般为23
+ POP3/SMTP代理: 主要用于以POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25
+ SOCKS代理: 只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080

##### 根据匿名程度区分

+ 高度匿名代理: 数据包会原封不动地转发
+ 普通匿名代理: 数据包会做一些处理
+ 透明代理: 会告诉服务器客户端真实的IP
+ 间谍代理: 由组织或个人创建的代理服务器

##### 常见代理设置

+ 对于网上的免费代理，最好使用高度代理，可以在使用前把所有代理都抓取下来筛选一遍拿到可用代理，也可以进一步维护一个代理池
+ 使用付费代理服务，使用付费的代理会好用很多
+ ADSL拨号，拨一次号换一次IP，稳定性高，也是一种比较有效的方法
+ 蜂窝代理，使用4G或5G网卡等制作的代理，由于使用蜂窝网络作为代理的情形比较少，因此整体被封锁的概率会比较低，但搭建蜂窝代理的成本是比较高的

## 多线程和多进程的基本原理

### 多线程的含义

先说说什么是进程:

进程可以理解为一个可以独立运行的程序单位，例如打开一个浏览器，这就是开启了一个浏览器进程；在一个进程中可以同时处理很多事情，比如浏览器可以打开很多个选项卡，这一个个选项卡其实就是一个个线程，**进程就是线程的集合，进程是由一个或多个线程构成的，线程是操作系统进行运算调度的最小单位，是进程中的最小运行单位**

有了上面的铺垫，我们可以了解到，多线程就是一个进程中同时执行多个线程，上面的浏览器进程就是典型的多线程

### 并发和并行

我们知道，在计算机中运行一个程序，底层是通过处理器运行一条条指令来实现的

并发是指多个线程对应的多条指令被快速轮换地执行

并行是指同一时刻有多条指令在多个处理器上同时执行

例如，系统处理器需要同时运行多个线程，**如果系统处理器只有一个核，那它只能通过并发地方式来运行这些线程**，然而如果系统处理器有多个核，那么在一个核执行一个线程地同时，另一个核可以执行另一个线程，这样两个线程就实现了并行执行

### 多线程适用场景

如果任务不全是计算密集型任务，就可以使用多线程来提高程序的整体执行效率，尤其对于网络爬虫这种IO密集型任务，使用多线程能够大大提高程序整体的爬取效率

### 多进程的含义

顾名思义，**多进程就是同时运行多个进程**，由于进程就是线程的集合，而且进程是由一个或多个线程构成的，所以**多进程就意味着有大于等于进程数量的线程在同时运行**

### Python中的多线程和多进程

Python中GIL(全局解释器锁，其设计之初是出于对数据安全的考虑)的限制导致不论是在单核还是多核条件下，同一时刻都只能运行一个线程，这使得Python多线程无法发挥多核并行的优势

在Python的多线程下，每个线程的执行方式分如下三步:

+ 获取GIL
+ 执行对应线程的代码
+ 释放GIL

而对于多进程来说，每个进程都有属于自己的GIL，所以在多核处理器下，多进程的运行是不会受GIL影响的，也就是说，多进程能够更好地发挥多核优势

不过，对于爬虫这种IO密集型任务来说，多线程和多进程产生的影响差别并不大；但对于计算密集型任务来说，由于GIL的存在，Python多线程的整体运行效率在多核情况下可能反而比单核更低，而Python的多进程相比多线程，运行效率在多核的情况下比单核会有成倍提升

从整体来说，Python的多进程比多线程更有优势，所以，如果条件允许的话，尽量使用多进程





