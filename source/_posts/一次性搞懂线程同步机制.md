---
title: 一次性搞懂线程同步机制
date: 2022-1-12 15:11:30
---

使用线程，我们可以并发地在各个CPU核心上执行任务，最大化CPU的利用率，但线程也可能导致各种奇怪的资源竞争问题

[![7uaCgP.png](https://s4.ax1x.com/2022/01/12/7uaCgP.png)](https://imgtu.com/i/7uaCgP)

相信大家一定都看过这个经典案例

[![7uaMvV.png](https://s4.ax1x.com/2022/01/12/7uaMvV.png)](https://imgtu.com/i/7uaMvV)

用不同的线程去更新同一段内存数据，比如我们这里总共创建10个线程

[![7uaWxP.png](https://s4.ax1x.com/2022/01/12/7uaWxP.png)](https://imgtu.com/i/7uaWxP)

每个线程累加这个数字1000000次，运行程序你会发现得到的结果并不是预期的10000000，实际显示的数字可能会比它小很多，并且每次输出的结果还都不一样

[![7uaOx0.png](https://s4.ax1x.com/2022/01/12/7uaOx0.png)](https://imgtu.com/i/7uaOx0)

这里我会向大家解释为什么会出现这种情况，以及各种常见的线程同步机制

[![7udPi9.png](https://s4.ax1x.com/2022/01/12/7udPi9.png)](https://imgtu.com/i/7udPi9)

我们先来回顾一下刚才的这段程序，其实问题就出在数字累加的这行代码上，虽然在程序中我们简单地写作n++，但是当程序被编译成机器代码时，n++其实被翻译成三条不同的机器指令(machine code)

[![7uddij.png](https://s4.ax1x.com/2022/01/12/7uddij.png)](https://imgtu.com/i/7uddij)

它们分别是，将内存中的数据加载到CPU的寄存器eax中，然后将eax中的数据+1，最后再将计算结果写回内存，换句话说，这里的n++并不是原子操作(atomic operation)，原子操作指的是不能被继续拆分、或者被其他操作打断的指令，这里顺带解释一下，寄存器(Registers)是CPU内部的小型存储器

[![7uwgjP.png](https://s4.ax1x.com/2022/01/12/7uwgjP.png)](https://imgtu.com/i/7uwgjP)

用来临时存放计算数据，CPU中的运算都离不开寄存器，它们的容量非常有限，但读写速度会比内存快很多，回到刚才的代码，如果不同的线程按照顺序依次执行

比如线程A先将数据5读入寄存器，然后+1得到6并写回内存，然后线程B再将数据6读入寄存器，+1得到7并写回内存，这样没有任何问题，但问题在于线程是并发执行的

[![7u05qK.png](https://s4.ax1x.com/2022/01/12/7u05qK.png)](https://imgtu.com/i/7u05qK)

可能线程A还未将累加后的数据写会内存，线程B就已经开始读取数据到寄存器，这样线程B就会读到修改之前的旧数据，最后的结果是数据只被累加了一次，这个就是我们平时说的线程资源竞争而导致的数据不一致问题，要解决这个问题，我们需要对线程进行同步，也就是让原先异步的操作依次有序地执行，而锁(lock)是我们接下来要讲的第一种，也是最最基本的线程同步机制，它的概念非常简单，**在同一时间，只有一个线程可以获得(acquire)锁的拥有权(ownership)，此时其他的线程只能干等着，直到这个锁被持有者释放掉(release)**

[![7usuo8.png](https://s4.ax1x.com/2022/01/12/7usuo8.png)](https://imgtu.com/i/7usuo8)

锁的获取和释放有时候也被叫做上锁(lock)和解锁(unlock)，在不同的语言或者操作系统中，通常会用到不同的锁的实现，比如C++或者Go中的mutex互斥锁

[![7u2AbR.png](https://s4.ax1x.com/2022/01/12/7u2AbR.png)](https://imgtu.com/i/7u2AbR)

[![7u21rd.png](https://s4.ax1x.com/2022/01/12/7u21rd.png)](https://imgtu.com/i/7u21rd)

java则允许使用synchronized关键字来对某个函数、对象或者代码上锁

[![7uRQYV.png](https://s4.ax1x.com/2022/01/12/7uRQYV.png)](https://imgtu.com/i/7uRQYV)

当然更底层一些，我们甚至可以调用操作系统的API来实现锁的功能，其他的你可能听过的还包括自旋锁、读写锁等等

[![7uWpX4.png](https://s4.ax1x.com/2022/01/12/7uWpX4.png)](https://imgtu.com/i/7uWpX4)

总而言之，锁的核心概念是非常简单的

我们只需要记住在访问共享资源之前上锁，并在结束之后解锁即可

[![7uWDNq.png](https://s4.ax1x.com/2022/01/12/7uWDNq.png)](https://imgtu.com/i/7uWDNq)

修改运行程序，我们可以看到，这里输出了我们的预期结果10000000，虽然这是一个样例程序，但是频繁加锁和解锁的操作是非常低效的

[![7uWvVA.png](https://s4.ax1x.com/2022/01/12/7uWvVA.png)](https://imgtu.com/i/7uWvVA)

这样会完全打破线程的并发执行

其实我们完全可以在线程中创建一个临时变量做计算，然后再将最终的结果累加到全局的共享变量中，这样只有最后一个操作需要同步，而线程的主体仍能并发地执行，另外在使用锁的时候需要格外小心，**多个锁的嵌套使用很可能导致线程的死锁(deadlock)现象**

比如这里有两个线程和两把锁

[![7uhg0J.png](https://s4.ax1x.com/2022/01/12/7uhg0J.png)](https://imgtu.com/i/7uhg0J)

线程1先获取锁1再获取锁2，线程2刚好相反，先获取锁2在获取锁1，如果这俩个线程同时运行，恰好线程q先获取了锁1，然后线程2获取了锁2，然后线程1继续执行，由于锁2被线程2占用，所以线程1会被阻塞，同时由于锁1被线程1占用，所以线程2也会被阻塞

[![7u5Zad.png](https://s4.ax1x.com/2022/01/12/7u5Zad.png)](https://imgtu.com/i/7u5Zad)

于是线程1和线程2同时被阻塞，也就造成了线程的死锁，这里关键的问题在于上锁的顺序，如果我们让所有线程都按照同样的顺序上锁，其实是可以避免这种情况的，不过实际情况肯恩远比这个复杂，每个线程会用到不同的锁，并且加锁和解锁的操作分散在代码的各个角落，所以另一种做法是，干脆就用单个锁来保护所有的共享资源

[![7uoWDO.png](https://s4.ax1x.com/2022/01/12/7uoWDO.png)](https://imgtu.com/i/7uoWDO)

并且仅仅在访问资源的时候再去上锁，虽然这么做会损失掉一部分线程的并发性(concurrency)，但好处在于程序的逻辑会更容易维护

讲到这里，我们顺便提一下部分语言支持的atomic语法修饰，这是一种不使用锁(如果硬件支持)但依然能够解决资源竞争的方法，比如像之前简单的加减操作，在机器内部会直接翻译成硬件支持的原子操作

[![7u7Elt.png](https://s4.ax1x.com/2022/01/12/7u7Elt.png)](https://imgtu.com/i/7u7Elt)

[![7u7MkQ.png](https://s4.ax1x.com/2022/01/12/7u7MkQ.png)](https://imgtu.com/i/7u7MkQ)

也就说指令已经是不可拆分的最小步骤，因此不需要同步，它的效率通常比使用锁更高

另外建立在锁之上，线程中还有其它更复杂、更高级的同步机制，比如信号量、条件变量等等，虽然它们也可以用来保护共享资源，但更主要的用途是在线程中传递信号(signaling)，比如使用条件变量，你可以让线程进入等待，直到某个条件成立后再继续执行，这个条件可能是网络资源被成功加载，或者某项数据准备完毕等等

[![7uH4VU.png](https://s4.ax1x.com/2022/01/12/7uH4VU.png)](https://imgtu.com/i/7uH4VU)

而信号量则更加灵活一点，你可以先让所有的线程进行等待，但在同一时间内，只让特定数量的线程被唤醒

[![7ubCRA.png](https://s4.ax1x.com/2022/01/12/7ubCRA.png)](https://imgtu.com/i/7ubCRA)

在后面我将再向大家详细解释信号量和条件变量的工作原理，因为它们很容易和锁混淆，但实际用途却完全不一样





